(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{426:function(t,a,r){"use strict";r.r(a);var s=r(2),v=Object(s.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"强化学习笔记"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#强化学习笔记"}},[t._v("#")]),t._v(" 强化学习笔记")]),t._v(" "),a("h2",{attrs:{id:"综述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#综述"}},[t._v("#")]),t._v(" 综述")]),t._v(" "),a("h3",{attrs:{id:"背景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#背景"}},[t._v("#")]),t._v(" 背景")]),t._v(" "),a("p",[t._v("定位：和有监督学习，无监督学习相对"),a("br"),t._v("\n特点：无需数据集，自行尝试学习"),a("br"),t._v("\n应用：游戏"),a("br"),t._v("\n分类：\n有模型和无模型，区别在于智能体是否知道系统"),a("br"),t._v("\n基于奖励 最优奖励总和"),a("br"),t._v("\n基于策略 找到最优策略")]),t._v(" "),a("h3",{attrs:{id:"对象"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#对象"}},[t._v("#")]),t._v(" 对象：")]),t._v(" "),a("p",[t._v("代理 环境 动作 策略 状态 奖励 目标"),a("br"),t._v("\n设置好环境，通过状态来指导代理，代理通过马尔科夫决策过程，"),a("br"),t._v("\n做出正确动作，获得奖励，将动作执行先后顺序形成策略，达到目标结束"),a("br"),t._v("\n自动进行决策，连续决策，最大化积累reward")]),t._v(" "),a("h3",{attrs:{id:"决策"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#决策"}},[t._v("#")]),t._v(" 决策：")]),t._v(" "),a("p",[t._v("策略学习 详细指示 输入状态"),a("br"),t._v("\nQ-learning 让代理自行学习 输入状态和动作")]),t._v(" "),a("h3",{attrs:{id:"局限"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#局限"}},[t._v("#")]),t._v(" 局限：")]),t._v(" "),a("p",[t._v("数据训练量大——效率低性能差"),a("br"),t._v("\n领域特殊性——有特定的更优方案"),a("br"),t._v("\n奖励函数设计——主观，局部最优解")]),t._v(" "),a("h3",{attrs:{id:"难题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#难题"}},[t._v("#")]),t._v(" 难题：")]),t._v(" "),a("p",[t._v("无法收敛")]),t._v(" "),a("h3",{attrs:{id:"应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用：")]),t._v(" "),a("p",[t._v("小老鼠吃奶酪")])])}),[],!1,null,null,null);a.default=v.exports}}]);