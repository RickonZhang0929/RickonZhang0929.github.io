---
title: 机器学习笔记
date: 2017-08-20
tags:
 - 机器学习
categories:
 - 技术知识
---

# 机器学习笔记



## 参考网站
李宏毅机器学习笔记  `https://datawhalechina.github.io/leeml-notes/#/`  
吴恩达深度学习笔记  `https://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/`  
吴恩达机器学习代码  `https://www.heywhale.com/home/column/5dd7524c83b6ff002c786fff`   
easyAI 网站`https://easyai.tech/ai-definition-category/machine-learning/`

## 机器学习分类
机器利用数据学习逻辑和算法   
主要分类：

* 无监督：给数据，无标注，聚类问题
* 监督：给数据，给标注，分类或回归问题
* 强化：给状态，动作，奖励，获得动作策略

神经网络 是基于 逻辑回归（监督学习）   
深度学习 是基于 神经网络

## AI层级
基础：数据和硬件算力   
算法：机器学习和深度学习的相关算法   
方向：CV，NLP，语音，决策，搜索   
具体：机器翻译，语义理解   
方案：智能安防

## 简单代码流程
1·读取数据   
2·矩阵化和标准化处理   
3·算法函数   
4·设置参数的初始值   
5·代入计算结果   
6·可视化展示

## 决策树

信息熵   
不确定程度   
可能的不确定度的期望   
对概率进行映射   
满足非负，连续，独立事件概率相乘不确定度相加

三个性质作为不确定度   
-log（p）采用负对数

不确定度的期望是，不确定度乘概率求和   
（事件的期望是 事件乘概率求和）

一个事件拆解成不同事件   
求所有事件的不确定度的期望   
便能衡量事件的信息量的大小

直观感受是事件的发生的概率越小   
H的值越大说明信息越多   
H值越小说明更加确定   
差值是条件改变产生的信息量   
H可以直接量化信息

先将事件理解成一个概率分布的问题   
不确定度就是分布情况的复杂度   
熵最小时是完全确定的状态

对于二叉树而言   
给出样本的属性	   
机器要做的是   
1·将属性设定先后   
2·找到阈值   
比较不同属性正确率   
尽可能提高正确率   
这个过程就是减小熵

熵：随机变量的不确定度   
条件熵：一个条件下 随机变量的不确定度   
信息增益：熵-条件熵   
不确定性的减少程度，即为我们带来了多少有用信息

举例解释
X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。X的熵减去Y条件下X的熵，就是信息增益。具体解释：原本明天下雨的信息熵是2，条件熵是0.01（因为如果知道明天是阴天，那么下雨的概率很大，信息量少），这样相减后为1.99。在获得阴天这个信息后，下雨信息不确定性减少了1.99，不确定减少了很多，所以信息增益大。也就是说，阴天这个信息对明天下午这一推断来说非常重要。所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键，决策树就是这样来找特征的。

选信息增益最大的特征作为根节点   
特征选择 选取具有分类能力的属性   
决策树生成 选择特征作为节点，然后将取值作为子节点，直到特征被用完   
决策树剪枝 对抗过拟合，预剪枝，后剪枝，然后对比和原来的效果

## 支持向量机
SVM   
具体的推导过于抽象难以理解   
包含的参数有 支持向量x，超平面，间隔d   
目标函数是最大化边距   
即最优超平面

通过目标函数和约束条件进行化简   
通过对偶问题和核技巧和SMO得到W和b

## 学习经验
理解整体的含义   
像代价函数，BP等底层算法   
无需过多关注细节

无需过度关注算法和实现细节   
只需要明白算法的架构和原理    
   