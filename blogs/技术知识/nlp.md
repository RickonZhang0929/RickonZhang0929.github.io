---
title: NLP学习笔记
date: 2017-08-22
tags:
 - 机器学习
categories:
 - 技术知识
---

# nlp学习笔记
## 技术类型
语音识别ASR   
自然语言理解NLU   
对话管理DM   
自然语言生成NLG   
语音合成TTS

NLU和DM是核心

## 对话系统

场景：QA闲聊   
技术：文本生成，检索式闲聊，对话状态跟踪，上下文理解

### 方法：
句法式规则   
意图+槽位   
抽象出概念并组合   
关键词

基于检索的问答   
处理问题   
关键词   
q-q 检索召回  粗糙匹配   
q-r matching && ranking 精确匹配

基于知识图谱问答   
语义解析   
语义关联

基于阅读理解问答   
匹配式QA   
抽取式QA

## 具体方法技术
### nlp处理过程

词法分析   
中文分词   
词性标注   
命名实体识别

信息提取

文本分类，聚类

句法分析

语义分析   
词义消歧   
语义角色标注   
语义依存分析

语料库   
针对每一个环节都有相应的语料库

### 对话状态追踪DST
把对话过程当时一个序列模型   
记忆网络跟踪上下文

### 文本生成NLG
根据句法式规则   
选择+拼接



### NLU
意图识别intent   
实体识别entity   
槽位提取——命名实体识别NER   
序列标注问题

### 意图识别的方法
语义提取   
文本分类

实体，概念的实例    
效果 能够提取到意图和实体

### DM
决策过程   
输入：语义表达 当前状态   
输出：动作或者给出回应   
功能   
决策和回复

基于规则   
基于统计——强化学习   
基于神经网络——rasa   
需要大量的数据去训练

### GPT3
是一个文本生成模型   
需要经过训练，可开发出强大的功能   
有上千亿个参数的神经网络   
输入 一个序列   
输出 序列末尾加上单词   
过程 一系列的编码（单词转换为向量），计算预测值（transformer解码层）解码（向量转换为单词）处理   
重复过程得到完整的输出   
token 单个单词   
架构是基于transformer解码器模型   
学习到的内容编码成1750亿个参数   
计算每次运行时生成的 token   
预测主要就是大量的矩阵乘法   
非监督预训练和监督微调   
API完成了非监督预训练 pre-training   
目标任务进行监督微调 fine-tuning

### Transformer
引入了注意力机制

### Seq2Seq
（强调目的）   
特点是输入序列、输出序列

### Encoder-Decoder 模型
（强调方法）   
输入——向量——输出   
中间向量长度固定   
压缩——解压

Seq2Seq 属于 Encoder-Decoder   
图片，文字，语音——文字

### Ａttention 模型
编码为单个长度固定的向量序列

### RNN
Recurrent Neural Network   
循环神经网络   
有效的处理序列数据   
输入序列数据   
前面的输入会影响到后面的输出   
存在短期记忆问题   
以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接形成闭合回路的递归神经网络（recursive neural network）

### LSTM
Long short-term memory   
长短期记忆网络

LSTM是RNN的优化   
GRU是LSTM的优化

### 推荐系统
不具有目的性，依赖用户的历史行为和画像数据进行个性化推荐   
本质：信息过滤系统   
方法：召回、排序、重排序   
场景：   
资讯类：今日头条、腾讯新闻等   
电商类：淘宝、京东、拼多多、亚马逊等   
娱乐类：抖音、快手、爱奇艺等   
生活服务类：美团、大众点评、携程等   
社交类：微信、陌陌、脉脉等   
场景：   
基于用户维度的推荐：   
根据用户的历史行为和兴趣进行推荐，比如淘宝首页的猜你喜欢、抖音的首页推荐等。   
基于物品维度的推荐：   
根据用户当前浏览的标的物进行推荐，比如打开京东APP的商品详情页，会推荐和主商品相关的商品给你。

### Embedding
嵌入层   
将一个高维的对象表示成一个低维的向量   
方便处理对象之间的潜在关系   
   
   
   
   
   